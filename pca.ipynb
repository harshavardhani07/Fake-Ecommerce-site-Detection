{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243b828c-7dd5-4550-829c-332a14ad1177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        socialNbFollowers  socialNbFollows  socialProductsLiked  \\\n",
      "count                1207             1207                 1207   \n",
      "unique                 13               21                   93   \n",
      "top                     4                8                    0   \n",
      "freq                  207              896                  394   \n",
      "\n",
      "        productsListed  productsWished  productsBought  civilityGenderId  \\\n",
      "count             1207            1207            1207              1207   \n",
      "unique               6              61              26                 3   \n",
      "top                  1               0               0                 2   \n",
      "freq               499             728             862               881   \n",
      "\n",
      "       civilityTitle  hasAnyApp  hasAndroidApp  ...  countryCode_vg  \\\n",
      "count           1207       1207           1207  ...          1207.0   \n",
      "unique             3          2              2  ...             1.0   \n",
      "top              mrs          1              0  ...             0.0   \n",
      "freq             881        763           1077  ...          1207.0   \n",
      "\n",
      "        countryCode_vi  countryCode_vn  countryCode_vu  countryCode_ws  \\\n",
      "count           1207.0          1207.0          1207.0          1207.0   \n",
      "unique             1.0             1.0             1.0             1.0   \n",
      "top                0.0             0.0             0.0             0.0   \n",
      "freq            1207.0          1207.0          1207.0          1207.0   \n",
      "\n",
      "        countryCode_yt  countryCode_za  countryCode_zm  countryCode_zw   Fraud  \n",
      "count           1207.0          1207.0          1207.0          1207.0  1207.0  \n",
      "unique             1.0             1.0             1.0             1.0     2.0  \n",
      "top                0.0             0.0             0.0             0.0     0.0  \n",
      "freq            1207.0          1207.0          1207.0          1207.0   649.0  \n",
      "\n",
      "[4 rows x 423 columns]\n",
      "  socialNbFollowers socialNbFollows socialProductsLiked productsListed  \\\n",
      "0                15               8                   0              2   \n",
      "1                15               8                   0              2   \n",
      "2                15               8                   0              2   \n",
      "3                15               8                   0              2   \n",
      "4                15               8                   0              2   \n",
      "\n",
      "  productsWished productsBought civilityGenderId civilityTitle hasAnyApp  \\\n",
      "0              0              0                1            mr         1   \n",
      "1              0              0                1            mr         1   \n",
      "2              0              0                1            mr         1   \n",
      "3              0              0                1            mr         1   \n",
      "4              0              0                1            mr         1   \n",
      "\n",
      "  hasAndroidApp  ... countryCode_vg countryCode_vi countryCode_vn  \\\n",
      "0             0  ...            0.0            0.0            0.0   \n",
      "1             0  ...            0.0            0.0            0.0   \n",
      "2             0  ...            0.0            0.0            0.0   \n",
      "3             0  ...            0.0            0.0            0.0   \n",
      "4             0  ...            0.0            0.0            0.0   \n",
      "\n",
      "  countryCode_vu countryCode_ws countryCode_yt countryCode_za countryCode_zm  \\\n",
      "0            0.0            0.0            0.0            0.0            0.0   \n",
      "1            0.0            0.0            0.0            0.0            0.0   \n",
      "2            0.0            0.0            0.0            0.0            0.0   \n",
      "3            0.0            0.0            0.0            0.0            0.0   \n",
      "4            0.0            0.0            0.0            0.0            0.0   \n",
      "\n",
      "  countryCode_zw Fraud  \n",
      "0            0.0     0  \n",
      "1            0.0     0  \n",
      "2            0.0     0  \n",
      "3            0.0     1  \n",
      "4            0.0     1  \n",
      "\n",
      "[5 rows x 423 columns]\n",
      "X_train shape: (965, 422)\n",
      "y_train shape: (965,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "data = pd.read_csv(\"raw_data.csv\")\n",
    "# print(data.columns)\n",
    "if 'identifierHash' in data.columns:\n",
    "    data.drop('identifierHash', axis=1, inplace=True)\n",
    "\n",
    "#data.drop(['identifierHash','type','country','language','hasAnyApp','civilityTitle','civilityGenderId','seniorityAsMonths','seniorityAsYears','countryCode','productsWished','productsBought','hasAndroidApp','hasIosApp'],axis=1,inplace=True)\n",
    "# Initialize encoders\n",
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "#print(data['gender'].unique())\n",
    "converted = pd.get_dummies(data['gender'], drop_first=1)\n",
    "data = pd.concat([data, converted], axis = 1)\n",
    "data.drop('gender',axis = 1, inplace=True)\n",
    "data.rename(columns={'M': 'Male'}, inplace=True)\n",
    "#print(data['Male'].unique())\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Check the data types of each column in the DataFrame\n",
    "#print(data.dtypes)\n",
    "\n",
    "# Identify numeric columns (int and float types)\n",
    "numeric_columns = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "boolean_columns = ['hasAnyApp', 'hasAndroidApp', 'hasIosApp', 'hasProfilePicture', 'Male']\n",
    "# Handle non-numeric columns separately (e.g., keep them as they are, or convert them to category type)\n",
    "# Example: Convert object-type columns to category type if necessary\n",
    "object_columns = data.select_dtypes(include='object').columns\n",
    "for col in object_columns:\n",
    "    data[col] = data[col].astype('category')\n",
    "# Convert boolean columns to integers\n",
    "for col in boolean_columns:\n",
    "    data[col] = data[col].astype(int)\n",
    "\n",
    "# Convert only numeric columns to integer types\n",
    "data[numeric_columns] = data[numeric_columns].astype(int)\n",
    "\n",
    "categorical_columns = ['type', 'country', 'language', 'countryCode']\n",
    "# Convert categorical columns using one-hot encoding\n",
    "encoded_data = one_hot_encoder.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Create a DataFrame with one-hot encoded columns\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Combine the one-hot encoded columns with the original data, excluding the original categorical columns\n",
    "data = pd.concat([data, encoded_df], axis=1).drop(categorical_columns, axis=1)\n",
    "\n",
    "def zscore(array):\n",
    "    thr = 3\n",
    "    mean = np.mean(array)\n",
    "    std = np.std(array)\n",
    "    z_scores = (array - mean) / std\n",
    "    return np.abs(z_scores) > thr\n",
    "\n",
    "\n",
    "combined_condition = ~(zscore(data['socialNbFollows']) | zscore(data['socialNbFollowers']) | zscore(data['productsListed']) | zscore(data['productsSold']) | zscore(data['socialProductsLiked']))\n",
    "data = data[combined_condition]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "#print(new_data.head())\n",
    "#print(data.describe())\n",
    "\n",
    "\n",
    "def pure_round(num):\n",
    "    integer = int(num)\n",
    "    fraction = num - float(integer)\n",
    "    if fraction >= 0.5:\n",
    "        integer += 1\n",
    "    return integer\n",
    "\n",
    "data = data[data['productsListed'] != 0]\n",
    "for i in data.index:\n",
    "    case_no = data.loc[i,'productsSold']\n",
    "    pass_no = pure_round((case_no * data.loc[i,'productsPassRate']) / 100)\n",
    "    fail_no = case_no - pass_no\n",
    "    data.loc[i,'productsPassed'] = pass_no\n",
    "    data.loc[i,'productsFailed'] = fail_no\n",
    "    if case_no == 0:\n",
    "        data.drop(i, axis=0, inplace=True)\n",
    " \n",
    "data.drop(['productsPassRate','productsSold'], axis=1, inplace=True)\n",
    "\n",
    "# Encodedict = {}\n",
    "# for i in ['Male','hasProfilePicture']:\n",
    "#     key = '_{}'.format(i)\n",
    "#     le = LabelEncoder()\n",
    "#     data[key] = le.fit_transform(list(data[i]))\n",
    "#     Encodedict[key] = le.classes_\n",
    "\n",
    "# data.drop(['Male','hasProfilePicture'], axis=1, inplace=True)\n",
    "\n",
    "dfdict = {}\n",
    "for j in data.index:\n",
    "    x = data.loc[j,'productsPassed']\n",
    "    y = data.loc[j,'productsFailed']\n",
    "    if x != 0:\n",
    "        data.loc[j,'Fraud'] = 0\n",
    "        df = pd.DataFrame(data.loc[j,:]).transpose()\n",
    "        ldf = pd.concat([df]*int(x), ignore_index=True)\n",
    "    \n",
    "    if y != 0:\n",
    "        data.loc[j,'Fraud'] = 1\n",
    "        df2 = pd.DataFrame(data.loc[j,:]).transpose()\n",
    "        ldf2 = pd.concat([df2]*int(y), ignore_index=True)\n",
    "    \n",
    "    if x != 0 and y != 0:\n",
    "        dfdict[j] = pd.concat([ldf, ldf2], ignore_index=True)\n",
    "    elif x != 0:\n",
    "        dfdict[j] = ldf\n",
    "    else:\n",
    "        dfdict[j] = ldf2\n",
    "\n",
    "data_new = pd.concat(dfdict.values(), ignore_index=True)\n",
    "data_new.drop(['productsPassed','productsFailed'], axis=1, inplace=True)\n",
    "print(data_new.describe())\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'Fraud' is your target variable\n",
    "X = data_new.drop('Fraud', axis=1)  # Features\n",
    "y = data_new['Fraud']  # Target variable\n",
    "# Step 1: Convert the 'Fraud' column to numeric values, coercing any non-numeric values to NaN\n",
    "data_new['Fraud'] = pd.to_numeric(data_new['Fraud'], errors='coerce')\n",
    "\n",
    "# Step 2: Convert the 'Fraud' column to int64\n",
    "data_new['Fraud'] = data_new['Fraud'].astype('int64')\n",
    "plt.figure(figsize=(4, 4))\n",
    "#sns.countplot(x='productsListed', hue='Fraud', data=data_new, palette = 'inferno')\n",
    "#sns.boxplot(x='productsListed', y='socialNbFollowers', hue='Fraud', data=data, palette = 'inferno')\n",
    "sns.histplot(x = 'seniority', data = data_new, binwidth = 350, hue = 'Fraud', element = 'step')\n",
    "# Splitting the data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optionally, you can reset index for train and test sets\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "print(data_new.head())\n",
    "# Ensure y_train contains only numeric values, and convert non-numeric values to NaN\n",
    "y_train = pd.to_numeric(y_train, errors='coerce')\n",
    "\n",
    "# Handle any missing values (NaN) as needed\n",
    "# For example, you can drop rows with missing values or fill them with a default value:\n",
    "y_train = y_train.dropna()  # Drop rows with NaN values\n",
    "\n",
    "# Convert the series to integer type\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "y_test = y_test.astype(int)\n",
    "X_test.fillna(X_test.mean(), inplace=True)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42dd3c07-6004-4b75-8ff3-c4efbdf982c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test data type: int64\n",
      "y_pred data type: int64\n",
      "Unique values in y_test: [0 1]\n",
      "Unique values in y_pred: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"y_test data type: {y_test.dtype}\")\n",
    "print(f\"y_pred data type: {y_pred.dtype}\")\n",
    "\n",
    "print(f\"Unique values in y_test: {np.unique(y_test)}\")\n",
    "print(f\"Unique values in y_pred: {np.unique(y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29b1f40e-e133-49ad-8ed9-92ff6ff1dc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without PCA: 0.4380165289256198\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.24      0.33       136\n",
      "           1       0.41      0.69      0.52       106\n",
      "\n",
      "    accuracy                           0.44       242\n",
      "   macro avg       0.46      0.47      0.42       242\n",
      "weighted avg       0.46      0.44      0.41       242\n",
      "\n",
      "Accuracy with PCA: 0.5289256198347108\n",
      "Classification Report with PCA:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55       136\n",
      "           1       0.47      0.54      0.50       106\n",
      "\n",
      "    accuracy                           0.53       242\n",
      "   macro avg       0.53      0.53      0.53       242\n",
      "weighted avg       0.54      0.53      0.53       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries and modules\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define the imputation strategy for numerical and categorical features\n",
    "numeric_transformer = SimpleImputer(strategy='mean')  # Use mean imputation for numerical data\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent')  # Use most frequent for categorical data\n",
    "\n",
    "# Define which features are numerical and categorical\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a ColumnTransformer to handle different feature types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline with the preprocessor and the GaussianNB classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', GaussianNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use the pipeline to predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print the classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy without PCA:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Step 1: Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')  # Choose a strategy (mean, median, most_frequent)\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Step 2: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA(n_components=0.93)  # Specify the desired variance to retain\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Step 4: Train and evaluate Gaussian Naive Bayes with PCA\n",
    "gnb_pca = GaussianNB()\n",
    "gnb_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = gnb_pca.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_with_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(\"Accuracy with PCA:\", accuracy_with_pca)\n",
    "print(\"Classification Report with PCA:\")\n",
    "print(classification_report(y_test, y_pred_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "499e85dd-4e25-4b25-ba72-60e87b3bbbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest with PCA:\n",
      "Accuracy: 0.7355371900826446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77       136\n",
      "           1       0.72      0.65      0.68       106\n",
      "\n",
      "    accuracy                           0.74       242\n",
      "   macro avg       0.73      0.73      0.73       242\n",
      "weighted avg       0.73      0.74      0.73       242\n",
      "\n",
      "\n",
      "Random Forest without PCA:\n",
      "Accuracy: 0.7355371900826446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       136\n",
      "           1       0.71      0.66      0.69       106\n",
      "\n",
      "    accuracy                           0.74       242\n",
      "   macro avg       0.73      0.73      0.73       242\n",
      "weighted avg       0.73      0.74      0.73       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Handle missing values and categorical data with ColumnTransformer\n",
    "numeric_features = X_train.columns.difference(['civilityTitle'])  # Update with your actual column names\n",
    "categorical_features = ['civilityTitle']  # Update with your actual column names\n",
    "\n",
    "numeric_transformer = SimpleImputer(strategy='mean')  # Use mean imputation for numeric data\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent')  # Use most frequent for categorical data\n",
    "\n",
    "# Create ColumnTransformer with both transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a pipeline with the preprocessor and PCA\n",
    "pipeline_pca = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=100)),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "# Train the pipeline with PCA on the training data\n",
    "pipeline_pca.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_pca = pipeline_pca.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "report_pca = classification_report(y_test, y_pred_pca)\n",
    "\n",
    "print(\"\\nRandom Forest with PCA:\")\n",
    "print(\"Accuracy:\", accuracy_pca)\n",
    "print(report_pca)\n",
    "\n",
    "# Define a second pipeline without PCA for comparison\n",
    "pipeline_original = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "# Train the pipeline without PCA on the training data\n",
    "pipeline_original.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_original = pipeline_original.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "report_original = classification_report(y_test, y_pred_original)\n",
    "\n",
    "print(\"\\nRandom Forest without PCA:\")\n",
    "print(\"Accuracy:\", accuracy_original)\n",
    "print(report_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "74f53741-57d5-4c8c-936e-6eed9bf3b2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest with regularization:\n",
      "Accuracy: 0.640495867768595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71       136\n",
      "           1       0.62      0.46      0.53       106\n",
      "\n",
      "    accuracy                           0.64       242\n",
      "   macro avg       0.64      0.62      0.62       242\n",
      "weighted avg       0.64      0.64      0.63       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define parameters for Random Forest\n",
    "n_estimators = 100  # Number of trees in the forest\n",
    "max_depth = 10  # Maximum depth of each tree (you can experiment with this value)\n",
    "min_samples_split = 4  # Minimum samples required to split a node\n",
    "min_samples_leaf = 2  # Minimum samples required at a leaf node\n",
    "max_features = 'sqrt'  # Number of features to consider for best split\n",
    "\n",
    "# Create a pipeline with the preprocessor and Random Forest with regularization\n",
    "pipeline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42  # Seed for reproducibility\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest with regularization:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(report_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "da0a92e0-ae62-4fbf-a7f4-c88160ca89d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression with PCA:\n",
      "Accuracy: 0.5578512396694215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63       136\n",
      "           1       0.49      0.40      0.44       106\n",
      "\n",
      "    accuracy                           0.56       242\n",
      "   macro avg       0.54      0.54      0.54       242\n",
      "weighted avg       0.55      0.56      0.55       242\n",
      "\n",
      "\n",
      "Logistic Regression without PCA:\n",
      "Accuracy: 0.5909090909090909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.73      0.67       136\n",
      "           1       0.54      0.42      0.47       106\n",
      "\n",
      "    accuracy                           0.59       242\n",
      "   macro avg       0.58      0.57      0.57       242\n",
      "weighted avg       0.58      0.59      0.58       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# Define a pipeline with the preprocessor, scaler, PCA, and Logistic Regression\n",
    "pipeline_pca_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.80)),  # Adjust n_components as needed\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the pipeline with PCA on the training data\n",
    "pipeline_pca_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_pca_lr = pipeline_pca_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_pca_lr = accuracy_score(y_test, y_pred_pca_lr)\n",
    "report_pca_lr = classification_report(y_test, y_pred_pca_lr)\n",
    "\n",
    "print(\"\\nLogistic Regression with PCA:\")\n",
    "print(\"Accuracy:\", accuracy_pca_lr)\n",
    "print(report_pca_lr)\n",
    "\n",
    "# Define a second pipeline without PCA for comparison\n",
    "pipeline_original_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the pipeline without PCA on the training data\n",
    "pipeline_original_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_original_lr = pipeline_original_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_original_lr = accuracy_score(y_test, y_pred_original_lr)\n",
    "report_original_lr = classification_report(y_test, y_pred_original_lr)\n",
    "\n",
    "print(\"\\nLogistic Regression without PCA:\")\n",
    "print(\"Accuracy:\", accuracy_original_lr)\n",
    "print(report_original_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "feaed7f7-1407-47df-934d-a0c08279ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression without PCA but with regularization:\n",
      "Accuracy: 0.5909090909090909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.73      0.67       136\n",
      "           1       0.54      0.42      0.47       106\n",
      "\n",
      "    accuracy                           0.59       242\n",
      "   macro avg       0.58      0.57      0.57       242\n",
      "weighted avg       0.58      0.59      0.58       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the regularization type (penalty) and regularization strength (C)\n",
    "# 'l1' for L1 regularization and 'l2' for L2 regularization\n",
    "penalty_type = 'l2'  # Change to 'l1' if you want L1 regularization\n",
    "C_value = 1.0  # Regularization strength; you can experiment with different values\n",
    "\n",
    "# Define the logistic regression model with the appropriate solver for L1 regularization\n",
    "solver_type = 'liblinear' if penalty_type == 'l1' else 'lbfgs'\n",
    "\n",
    "# Create a pipeline without PCA, including preprocessing and logistic regression\n",
    "pipeline_no_pca_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Column transformer for data preprocessing\n",
    "    ('scaler', StandardScaler()),  # Standard scaling (optional but recommended)\n",
    "    ('clf', LogisticRegression(\n",
    "        penalty=penalty_type,\n",
    "        C=C_value,\n",
    "        solver=solver_type,\n",
    "        max_iter=1000,\n",
    "        random_state=42  # Seed for reproducibility\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline with logistic regression (no PCA) on the training data\n",
    "pipeline_no_pca_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_no_pca_lr = pipeline_no_pca_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_no_pca_lr = accuracy_score(y_test, y_pred_no_pca_lr)\n",
    "report_no_pca_lr = classification_report(y_test, y_pred_no_pca_lr)\n",
    "\n",
    "print(\"\\nLogistic Regression without PCA but with regularization:\")\n",
    "print(\"Accuracy:\", accuracy_no_pca_lr)\n",
    "print(report_no_pca_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "906f6124-aa29-4853-85e6-f65c5fe6a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression with PCA:\n",
      "Accuracy: 0.5578512396694215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63       136\n",
      "           1       0.49      0.40      0.44       106\n",
      "\n",
      "    accuracy                           0.56       242\n",
      "   macro avg       0.54      0.54      0.54       242\n",
      "weighted avg       0.55      0.56      0.55       242\n",
      "\n",
      "\n",
      "Logistic Regression with PCA and regularization:\n",
      "Accuracy: 0.5909090909090909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.73      0.67       136\n",
      "           1       0.54      0.42      0.47       106\n",
      "\n",
      "    accuracy                           0.59       242\n",
      "   macro avg       0.58      0.57      0.57       242\n",
      "weighted avg       0.58      0.59      0.58       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# Define a pipeline with the preprocessor, scaler, PCA, and Logistic Regression\n",
    "pipeline_pca_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=0.80)),  # Adjust n_components as needed\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the pipeline with PCA on the training data\n",
    "pipeline_pca_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_pca_lr = pipeline_pca_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_pca_lr = accuracy_score(y_test, y_pred_pca_lr)\n",
    "report_pca_lr = classification_report(y_test, y_pred_pca_lr)\n",
    "\n",
    "print(\"\\nLogistic Regression with PCA:\")\n",
    "print(\"Accuracy:\", accuracy_pca_lr)\n",
    "print(report_pca_lr)\n",
    "# Define the regularization type (penalty) and regularization strength (C)\n",
    "# 'l1' for L1 regularization and 'l2' for L2 regularization\n",
    "penalty_type = 'l2'  # Change to 'l2' if you want L2 regularization\n",
    "C_value = 1.0  # Regularization strength; you can experiment with different values\n",
    "\n",
    "# Define the logistic regression model with the appropriate solver for L1 regularization\n",
    "solver_type = 'liblinear' if penalty_type == 'l1' else 'lbfgs'\n",
    "\n",
    "# Create a pipeline with preprocessing, PCA, and logistic regression\n",
    "pipeline_pca_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=100)),  # Adjust the number of components as needed\n",
    "    ('clf', LogisticRegression(\n",
    "        penalty=penalty_type,\n",
    "        C=C_value,\n",
    "        solver=solver_type,\n",
    "        max_iter=1000,\n",
    "        random_state=42  # Seed for reproducibility\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the pipeline with PCA and logistic regression on the training data\n",
    "pipeline_pca_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_pca_lr = pipeline_pca_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_pca_lr = accuracy_score(y_test, y_pred_pca_lr)\n",
    "report_pca_lr = classification_report(y_test, y_pred_pca_lr)\n",
    "\n",
    "print(\"\\nLogistic Regression with PCA and regularization:\")\n",
    "print(\"Accuracy:\", accuracy_pca_lr)\n",
    "print(report_pca_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69871a12-9053-4911-973a-42f391b64899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-Nearest Neighbors with PCA:\n",
      "Accuracy: 0.5950413223140496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.62      0.63       136\n",
      "           1       0.54      0.57      0.55       106\n",
      "\n",
      "    accuracy                           0.60       242\n",
      "   macro avg       0.59      0.59      0.59       242\n",
      "weighted avg       0.60      0.60      0.60       242\n",
      "\n",
      "\n",
      "K-Nearest Neighbors without PCA:\n",
      "Accuracy: 0.6033057851239669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.62      0.64       136\n",
      "           1       0.54      0.58      0.56       106\n",
      "\n",
      "    accuracy                           0.60       242\n",
      "   macro avg       0.60      0.60      0.60       242\n",
      "weighted avg       0.61      0.60      0.60       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "/home/harsha/.local/lib/python3.10/site-packages/sklearn/impute/_base.py:577: UserWarning: Skipping features without any observed values: ['civilityTitle']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Define a pipeline with the preprocessor, scaler, PCA, and K-Nearest Neighbors\n",
    "pipeline_pca_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=100)),  # Adjust n_components as needed\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Train the pipeline with PCA on the training data\n",
    "pipeline_pca_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_pca_knn = pipeline_pca_knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_pca_knn = accuracy_score(y_test, y_pred_pca_knn)\n",
    "report_pca_knn = classification_report(y_test, y_pred_pca_knn)\n",
    "\n",
    "print(\"\\nK-Nearest Neighbors with PCA:\")\n",
    "print(\"Accuracy:\", accuracy_pca_knn)\n",
    "print(report_pca_knn)\n",
    "\n",
    "# Define a second pipeline without PCA for comparison\n",
    "pipeline_original_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Train the pipeline without PCA on the training data\n",
    "pipeline_original_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data using the pipeline\n",
    "y_pred_original_knn = pipeline_original_knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy_original_knn = accuracy_score(y_test, y_pred_original_knn)\n",
    "report_original_knn = classification_report(y_test, y_pred_original_knn)\n",
    "\n",
    "print(\"\\nK-Nearest Neighbors without PCA:\")\n",
    "print(\"Accuracy:\", accuracy_original_knn)\n",
    "print(report_original_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c25f12-fc33-43ce-9396-b586527fe9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
